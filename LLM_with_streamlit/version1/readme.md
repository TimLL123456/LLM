# Build frontend with Streamlit for LLM

### Use Ollama as LLM server
![image](https://github.com/Timlung1234/LLM/assets/156076725/935fef60-02ee-4b81-9183-e66efc51fa08)
