{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Ollama Server by Openai() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "- Structured Outputs with Ollama: https://github.com/jxnl/instructor/blob/main/docs/examples/ollama.md\n",
    "- Controlling Large Language Model Output with Pydantic: https://medium.com/@mattchinnock/controlling-large-language-model-output-with-pydantic-74b2af5e79d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m51 packages\u001b[0m in 4.10s\u001b[0m\n",
      "\u001b[2mDownloaded \u001b[1m17 packages\u001b[0m in 3.33s\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m26 packages\u001b[0m in 285ms\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.9.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1masync-timeout\u001b[0m\u001b[2m==4.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.12.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdocstring-parser\u001b[0m\u001b[2m==0.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozendict\u001b[0m\u001b[2m==2.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhtml5lib\u001b[0m\u001b[2m==1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1minstructor\u001b[0m\u001b[2m==1.2.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==5.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.0.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultitasking\u001b[0m\u001b[2m==0.0.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==1.30.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeewee\u001b[0m\u001b[2m==3.17.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.18.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msoupsieve\u001b[0m\u001b[2m==2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.66.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper\u001b[0m\u001b[2m==0.12.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebencodings\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myfinance\u001b[0m\u001b[2m==0.2.40\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!uv pip install pydantic instructor openai yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import yfinance as yf\n",
    "import instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = \"Google\"\n",
    "\n",
    "class StockInfo(BaseModel):\n",
    "    company: str = Field(..., description=\"Name of the company\")\n",
    "    ticker: str = Field(..., description=\"Ticker symbol of the company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables `response_model` in create call\n",
    "client = instructor.from_openai(\n",
    "    OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\",  # required, but unused\n",
    "    ),\n",
    "    mode=instructor.Mode.JSON,\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"llama3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Return the company name and the ticker symbol of the {company}.\"\n",
    "        }\n",
    "    ],\n",
    "    response_model=StockInfo,\n",
    "    max_retries=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stock price of the Google LLC is 177.85000610351562. USD\n"
     ]
    }
   ],
   "source": [
    "stock = yf.Ticker(resp.ticker)\n",
    "hist = stock.history(period=\"1d\")\n",
    "stock_price = hist['Close'].iloc[-1]\n",
    "print(f\"The stock price of the {resp.company} is {stock_price}. USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(BaseModel):\n",
    "    date: str = Field(..., description=\"Date of booking\")\n",
    "    period: str = Field(..., description=\"Period of booking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test(date='None', period='None')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = instructor.from_openai(\n",
    "    OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"Ollama\"\n",
    "    ),\n",
    "    mode=instructor.Mode.JSON,\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Instruction:\n",
    "Return the date and period based on the user input. You need to take the example as reference to answer question\n",
    "if no date or no period return None\n",
    "\n",
    "### Example\n",
    "Qurestion: Any Booking available at 2/2/2024\n",
    "return: {'date'='2-2-2024', 'period'='None'}\n",
    "\n",
    "### Example of irrelevant query\n",
    "Qurestion: Good morning\n",
    "return: {'date'='None', 'period'='None'}\n",
    "\n",
    "Query:\n",
    "I want to book service\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    response_model=test,\n",
    "    max_retries=5,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3\",\n",
    "                 keep_alive=-1,\n",
    "                 temperature=0,\n",
    "                 max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Thank you for reaching out to us. To proceed with booking a room, could you please provide me with a specific date and period? For example, what dates are you looking at (start and end dates)? This will help me assist you better.\n"
     ]
    }
   ],
   "source": [
    "content = \"\"\"\n",
    "You are a helpful agent for company to handle booking room with client.\n",
    "You need to help client finish the booking progress by attaining a correct date and period from the customer\n",
    "Please follow the example to guide client to respond\n",
    "\n",
    "### Example: unclear date and period\n",
    "Client: I want to book room\n",
    "AI: what is the date and period for the room?\n",
    "\n",
    "### Example: unclear date\n",
    "Client: I want to book next friday\n",
    "AI: Please provide me a clear date e.g. 2023/05/02\n",
    "\n",
    "Query:\n",
    "Hi\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=content\n",
    "    )\n",
    "]\n",
    "\n",
    "chat_model_response = llm.invoke(messages)\n",
    "print(chat_model_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here is a 50-word article on Large Language Models (LLMs):\\n\\nLarge Language Models (LLMs) are artificial intelligence systems that can process and understand vast amounts of text data. These models, such as BERT and RoBERTa, have revolutionized natural language processing by enabling applications like language translation, sentiment analysis, and text summarization. LLMs learn from massive datasets and can generate human-like responses, making them a game-changer for industries like customer service, marketing, and healthcare.' response_metadata={'model': 'llama3', 'created_at': '2024-05-23T03:51:12.386885257Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4483107130, 'load_duration': 2651963075, 'prompt_eval_count': 21, 'prompt_eval_duration': 100555000, 'eval_count': 102, 'eval_duration': 1679018000} id='run-61a345a8-c37c-43d9-a872-49ebf5deb24f-0'\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Write me a 50 word article on {topic}\")\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\"topic\":\"LLMs\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
